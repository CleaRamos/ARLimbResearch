# ARLimbResearch
This project simulates limb movement in XR for amputees by translating muscle sensor data into virtual limb motions.

## h1 Overview
Machine learning (ML) has enabled wearable devices to recognize hand gestures from surface electromyography (sEMG) signals. These devices create a new paradigm of human-computer interaction in extended reality (XR), which could be extended to the disabled group, particularly amputees with healthy muscles. This project explores how this technology can enable amputees to interact in XR and control digital devices in real life by investigating how to map the sEMG signals of an amputee to the target actions in VR and control sequences of a digital device.

This project will investigate the question in three phases. First, since no commercial product is available, the PI will work closely with Bucknell undergraduate students to develop a sEMG controller prototype. Current research prototype often uses a circular grid of 12 to 32 sEMG sensors. To obtain an optimal design, the research team will study and assess the number of required sensors and the best placement locations by quantifying the “independence-ness” of the signals using spectrum analysis. Next, the team will recruit participants (amputees with healthy muscles) via local connection Geisinger, create XR experiences for data collection, and use the collected data to train an ML model specifically for amputees. The team will also conduct a user study using a Likert scale questionnaire and task-based accuracy measurement in XR to evaluate the design. Based on the feedback of the user study, in the last phase, the researchers will refine and optimize the design for real-life applications.

## h1 Project Goals

## h2 Phase 1

## h3 Phase 1a - Prototype Development

## h3 Phase 1b - Optimal Design Investigation

## h2 Phase 2 - Refinement for Amputees

## h2 Phase 3 - Applications





## h1
Overview



## h1
Overview
